{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# VUEPOINT TEXT PROCESSING PIPELINE\n",
    "from vuepoint import NgramFreq\n",
    "from vuepoint.StatsPipeline import get_corpus_stats\n",
    "from vuepoint.TextPipeline import preprocess_corpus\n",
    "from vuepoint.TextPipeline import format_sent_topics\n",
    "from vuepoint.ModelingPipeline import process_words\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          27406\n",
       "unique         27081\n",
       "top       Thank you﻿\n",
       "freq              19\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_large = []\n",
    "for file in glob('downloads/cartrends/*.json'):\n",
    "    corpus = pd.read_json(file, encoding='utf-8', lines=True)\n",
    "    corpus_large.append(corpus)\n",
    "\n",
    "corpus_large = pd.concat(corpus_large, ignore_index=True)\n",
    "corpus_large.text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Happy New Year!\\n\\n⬇️Scotty’s Top DIY Tools:\\r...\n",
       "1    Hi Scotty I recently bought a 95 Camry automat...\n",
       "2    I really dislike those new grills as well.  Ha...\n",
       "3                             HAPPY NEW YEAR SCOTTY!!﻿\n",
       "4    Scotty Kilmer what do u think of a 2003 Chevy ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_large.text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting text features\n",
      "extracting content features from text\n",
      "extracting text sentiment features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    happy new year scotty top diy tool bluetooth s...\n",
       "1    hi scotty recently bought camry automatic v k ...\n",
       "2    really dislike new grill well happy new year s...\n",
       "4            scotty kilmer u think chevy thousand mile\n",
       "5    scotty kilmer happy new year always enjoy vide...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corpus_stats(corpus_large, 'text')\n",
    "corpus_large = corpus_large[corpus_large.word_count > 10]\n",
    "preprocess_corpus(corpus_large, 'text')\n",
    "corpus_large.text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_lengh</th>\n",
       "      <th>n_avg_word</th>\n",
       "      <th>n_numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.766548</td>\n",
       "      <td>209.290036</td>\n",
       "      <td>4.438942</td>\n",
       "      <td>0.302135</td>\n",
       "      <td>1.532800</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.471441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>44.481971</td>\n",
       "      <td>244.133382</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>0.783714</td>\n",
       "      <td>3.549292</td>\n",
       "      <td>0.295928</td>\n",
       "      <td>0.267065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>4.386364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>4.743330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.644555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1018.000000</td>\n",
       "      <td>5680.000000</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_count    char_lengh    n_avg_word    n_numerics    upper_case  \\\n",
       "count  16860.000000  16860.000000  16860.000000  16860.000000  16860.000000   \n",
       "mean      38.766548    209.290036      4.438942      0.302135      1.532800   \n",
       "std       44.481971    244.133382      0.601176      0.783714      3.549292   \n",
       "min       11.000000     28.000000      1.100000      0.000000      0.000000   \n",
       "25%       16.000000     85.000000      4.066667      0.000000      0.000000   \n",
       "50%       25.000000    134.000000      4.386364      0.000000      1.000000   \n",
       "75%       44.000000    237.000000      4.743330      0.000000      2.000000   \n",
       "max     1018.000000   5680.000000     15.384615     19.000000    224.000000   \n",
       "\n",
       "           polarity  subjectivity  \n",
       "count  16860.000000  16860.000000  \n",
       "mean       0.068759      0.471441  \n",
       "std        0.295928      0.267065  \n",
       "min       -1.000000      0.000000  \n",
       "25%       -0.050000      0.312500  \n",
       "50%        0.040000      0.500000  \n",
       "75%        0.225000      0.644555  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_large.to_csv('cartrends_large_corpus.csv')\n",
    "corpus_large.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                           16860\n",
       "unique                                          16826\n",
       "top       tesla autopilot safer human credible source\n",
       "freq                                                3\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_large.reindex(np.random.permutation(corpus_large.index))\n",
    "corpus_large.text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 3648),\n",
       " ('one', 2435),\n",
       " ('get', 2012),\n",
       " ('new', 1736),\n",
       " ('people', 1723),\n",
       " ('make', 1711),\n",
       " ('would', 1707),\n",
       " ('thing', 1686),\n",
       " ('look', 1611),\n",
       " ('year', 1585),\n",
       " ('drive', 1541),\n",
       " ('even', 1457),\n",
       " ('driving', 1392),\n",
       " ('time', 1328),\n",
       " ('driver', 1277),\n",
       " ('think', 1257),\n",
       " ('need', 1252),\n",
       " ('want', 1221),\n",
       " ('screen', 1221),\n",
       " ('know', 1212)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_unigrams = NgramFreq.get_top_unigrams(\n",
    "    corpus_large.text.values.tolist(), topn=20)\n",
    "top_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('look like', 309),\n",
       " ('touch screen', 291),\n",
       " ('start stop', 285),\n",
       " ('self driving', 279),\n",
       " ('best driver', 248),\n",
       " ('year old', 202),\n",
       " ('steering wheel', 181),\n",
       " ('low profile', 165),\n",
       " ('spare tire', 149),\n",
       " ('parking brake', 147),\n",
       " ('door handle', 145),\n",
       " ('sound like', 143),\n",
       " ('manual transmission', 137),\n",
       " ('worst trend', 137),\n",
       " ('motor trend', 136),\n",
       " ('feel like', 131),\n",
       " ('fake vent', 130),\n",
       " ('profile tire', 126),\n",
       " ('year ago', 126),\n",
       " ('stuck screen', 121)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams = NgramFreq.get_top_bigrams(\n",
    "    corpus_large.text.values.tolist(), topn=20)\n",
    "top_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('low profile tire', 116),\n",
       " ('tesla hater tesla', 92),\n",
       " ('hater tesla hater', 91),\n",
       " ('happy new year', 78),\n",
       " ('beep beep beep', 69),\n",
       " ('fake engine noise', 62),\n",
       " ('electronic parking brake', 59),\n",
       " ('take eye road', 45),\n",
       " ('new year scotty', 32),\n",
       " ('fake exhaust tip', 32),\n",
       " ('full size spare', 29),\n",
       " ('push button start', 29),\n",
       " ('auto start stop', 28),\n",
       " ('check engine light', 27),\n",
       " ('electronic door handle', 26),\n",
       " ('disciple jesus christ', 26),\n",
       " ('self driving vehicle', 25),\n",
       " ('front wheel drive', 24),\n",
       " ('keep eye road', 23),\n",
       " ('taking eye road', 22)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_trigrams = NgramFreq.get_top_trigrams(\n",
    "    corpus_large.text.values.tolist(), topn=20)\n",
    "top_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tesla hater tesla hater', 91),\n",
       " ('hater tesla hater tesla', 91),\n",
       " ('beep beep beep beep', 65),\n",
       " ('happy new year scotty', 32),\n",
       " ('peep peep peep peep', 14),\n",
       " ('world greatest drag race', 12),\n",
       " ('wheel low profile tire', 11),\n",
       " ('without taking eye road', 10),\n",
       " ('awakened truth disciple jesus', 10),\n",
       " ('truth disciple jesus christ', 10)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_quadgrams = NgramFreq.get_top_quadgrams(\n",
    "    corpus_large.text.values.tolist(), topn=10)\n",
    "top_quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['happy', 'new', 'year', 'scotty', 'top', 'diy', 'tool', 'bluetooth', 'scan', 'tool', 'cheap', 'scan', 'tool', 'professional', 'socket', 'set', 'wrench', 'set', 'charging', 'required', 'jump', 'starter', 'battery', 'pack', 'jump', 'starter', 'thing', 'used', 'video', 'common', 'sense', 'camera', 'camera', 'microphone', 'camera', 'tripod', 'computer', 'editing', 'uploading', 'video', 'editing', 'software', 'thumbnail', 'software', 'check', 'tool', 'use', 'highly', 'recommend', 'scotty', 'shirt', 'merch', 'subscribe', 'hit', 'notification', 'bell', 'scotty', 'social', 'facebook', 'instagram', 'twitter']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield(sent)\n",
    "\n",
    "data = corpus_large.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- **building the car trends model**\n",
    "\n",
    "- **data_ready**: final text processing pipeline with spaCy's `en_core_web_lg` model\n",
    "- **id2word**: build the dictionary from corpus\n",
    "- **corpus**: create the corpus **TDF** with Gensim `doc2bow`\n",
    "- **lda_model**: and finally we build the **LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.033*\"people\" + 0.031*\"want\" + 0.029*\"drive\" + 0.019*\"year\" + 0.014*\"old\" '\n",
      "  '+ 0.014*\"suv\" + 0.012*\"new\" + 0.011*\"road\" + 0.010*\"vehicle\" + 0.010*\"lot\"'),\n",
      " (1,\n",
      "  '0.020*\"buy\" + 0.015*\"thing\" + 0.011*\"good\" + 0.011*\"start\" + 0.011*\"engine\" '\n",
      "  '+ 0.009*\"vehicle\" + 0.009*\"need\" + 0.008*\"work\" + 0.008*\"stop\" + '\n",
      "  '0.008*\"know\"'),\n",
      " (2,\n",
      "  '0.029*\"look\" + 0.021*\"hate\" + 0.020*\"new\" + 0.019*\"feel\" + 0.015*\"love\" + '\n",
      "  '0.012*\"big\" + 0.011*\"think\" + 0.011*\"truck\" + 0.010*\"design\" + '\n",
      "  '0.009*\"headlight\"')]\n"
     ]
    }
   ],
   "source": [
    "data_ready = process_words(data_words)\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=3,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=10,\n",
    "                                            passes=10,\n",
    "                                            alpha='symmetric',\n",
    "                                            iterations=100,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_num</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_perc_contrib</th>\n",
       "      <th>key_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608</td>\n",
       "      <td>buy, thing, good, start, engine, vehicle, need...</td>\n",
       "      <td>[happy, new, year, scotty, diy, tool, bluetoot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815</td>\n",
       "      <td>buy, thing, good, start, engine, vehicle, need...</td>\n",
       "      <td>[recently, buy, camry, automatic, mile, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482</td>\n",
       "      <td>people, want, drive, year, old, suv, new, road...</td>\n",
       "      <td>[dislike, new, grill, happy, new, year, scotty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>buy, thing, good, start, engine, vehicle, need...</td>\n",
       "      <td>[think, mile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468</td>\n",
       "      <td>people, want, drive, year, old, suv, new, road...</td>\n",
       "      <td>[scotty_kilmer, happy, new, year, enjoy, video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_num  dominant_topic  topic_perc_contrib  \\\n",
       "0        0             1.0               0.608   \n",
       "1        1             1.0               0.815   \n",
       "2        2             0.0               0.482   \n",
       "3        3             1.0               0.663   \n",
       "4        4             0.0               0.468   \n",
       "\n",
       "                                           key_words  \\\n",
       "0  buy, thing, good, start, engine, vehicle, need...   \n",
       "1  buy, thing, good, start, engine, vehicle, need...   \n",
       "2  people, want, drive, year, old, suv, new, road...   \n",
       "3  buy, thing, good, start, engine, vehicle, need...   \n",
       "4  people, want, drive, year, old, suv, new, road...   \n",
       "\n",
       "                                                text  \n",
       "0  [happy, new, year, scotty, diy, tool, bluetoot...  \n",
       "1  [recently, buy, camry, automatic, mile, good, ...  \n",
       "2  [dislike, new, grill, happy, new, year, scotty...  \n",
       "3                                      [think, mile]  \n",
       "4  [scotty_kilmer, happy, new, year, enjoy, video...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = format_sent_topics(lda_model, corpus, data_ready, n_topics=3)\n",
    "dominant_topic = topic_keywords.reset_index()\n",
    "dominant_topic.columns = [\n",
    "    'doc_num', 'dominant_topic', 'topic_perc_contrib', 'key_words', 'text']\n",
    "\n",
    "dominant_topic.to_csv('dominant_3topic_model.csv')\n",
    "dominant_topic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_num</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_perc_contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "      <td>16860.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8429.500000</td>\n",
       "      <td>0.905160</td>\n",
       "      <td>0.597347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4867.207105</td>\n",
       "      <td>0.746916</td>\n",
       "      <td>0.142059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4214.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8429.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12644.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16859.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.969000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc_num  dominant_topic  topic_perc_contrib\n",
       "count  16860.000000    16860.000000        16860.000000\n",
       "mean    8429.500000        0.905160            0.597347\n",
       "std     4867.207105        0.746916            0.142059\n",
       "min        0.000000        0.000000            0.333000\n",
       "25%     4214.750000        0.000000            0.485000\n",
       "50%     8429.500000        1.000000            0.576000\n",
       "75%    12644.250000        1.000000            0.692000\n",
       "max    16859.000000        2.000000            0.969000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ego/anaconda3/envs/ai/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "pyLDAvis.save_html(vis, '3topic_ldamodel.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vocab length:  4380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48344830, 94703600)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "model = gensim.models.Word2Vec(size=100, \n",
    "                               window=5, \n",
    "                               min_count=5, \n",
    "                               sample=1e-4, \n",
    "                               seed=1,\n",
    "                               negative=5,\n",
    "                               workers=cpu_count(), sg=1)\n",
    "\n",
    "model.build_vocab(data_ready)\n",
    "print('model vocab length: ', len(model.wv.vocab))\n",
    "model.train(data_ready, total_examples=len(data_ready), epochs=400)\n",
    "# model.wv.save_word2vec_format(os.path.join('saved_models', 'model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gesture', 0.4699676036834717),\n",
       " ('commonly', 0.43160951137542725),\n",
       " ('gesture_control', 0.4015405774116516),\n",
       " ('trek', 0.38776665925979614),\n",
       " ('sex', 0.38675397634506226),\n",
       " ('indicate', 0.3791407644748688),\n",
       " ('difficulty', 0.37336450815200806),\n",
       " ('laggy', 0.3718671202659607),\n",
       " ('engage', 0.3712344765663147),\n",
       " ('transfer', 0.3711259961128235),\n",
       " ('command', 0.36593523621559143),\n",
       " ('volume', 0.36405009031295776),\n",
       " ('chemistry', 0.363465279340744),\n",
       " ('basic', 0.3623785078525543),\n",
       " ('idrive', 0.36224907636642456),\n",
       " ('frustrating', 0.3618530035018921),\n",
       " ('incident', 0.36155545711517334),\n",
       " ('gps', 0.36019399762153625),\n",
       " ('site', 0.3586016893386841),\n",
       " ('spread', 0.3554077446460724)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"voice_command\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attractive', 0.4177040457725525),\n",
       " ('leukemia', 0.3948470652103424),\n",
       " ('robot', 0.3864787817001343),\n",
       " ('bluetooth', 0.3769947588443756),\n",
       " ('medium', 0.3733176290988922),\n",
       " ('phone', 0.37244296073913574),\n",
       " ('usb', 0.36586570739746094),\n",
       " ('entry', 0.36483582854270935),\n",
       " ('vibrate', 0.36271965503692627),\n",
       " ('fiesta', 0.3607165515422821),\n",
       " ('navigation', 0.35931235551834106),\n",
       " ('device', 0.355599045753479),\n",
       " ('rename', 0.35479024052619934),\n",
       " ('wooden', 0.3541204035282135),\n",
       " ('addict', 0.3509850800037384),\n",
       " ('plug', 0.3505522608757019),\n",
       " ('cell', 0.35046106576919556),\n",
       " ('obtain', 0.3447158634662628),\n",
       " ('voice_control', 0.342851459980011),\n",
       " ('adult', 0.3369237780570984)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['play_music', 'command'], negative=['unwanted'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=10):\n",
    "    answers = model.wv.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control\n",
      "physical\n",
      "infotainment\n",
      "knob\n",
      "button\n",
      "operate\n",
      "command\n",
      "screen\n",
      "touchscreen\n",
      "eye\n"
     ]
    }
   ],
   "source": [
    "# so touch screen and a system and not want\n",
    "word_algebra(['touch_screen', 'system'], ['want'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg_room\n",
      "gesture_control\n",
      "disappointed\n",
      "seater\n",
      "volume\n",
      "gesture\n",
      "commonly\n",
      "citroen\n",
      "touchpad\n",
      "literal\n"
     ]
    }
   ],
   "source": [
    "word_algebra(['voice_control', 'voice_command', 'interface'], ['difficulty', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ego/anaconda3/envs/ai/lib/python3.6/site-packages/gensim/models/keyedvectors.py:858: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# from these words : 'safe' does not match the words in the string\n",
    "print(model.wv.doesnt_match(\"simple infotainment safe vehicle screen\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "touch\n"
     ]
    }
   ],
   "source": [
    "# by replacing screen with 'touch' then touch does not match the words in the string\n",
    "print(model.wv.doesnt_match(\"simple infotainment safe vehicle touch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
