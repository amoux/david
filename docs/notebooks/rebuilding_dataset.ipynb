{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import contractions\n",
    "\n",
    "from glob import glob\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dash', 0.8780766725540161),\n",
       " ('display', 0.8719692826271057),\n",
       " ('dashboard', 0.8458210229873657),\n",
       " ('button', 0.8425780534744263),\n",
       " ('infotainment', 0.8397122025489807),\n",
       " ('camera', 0.8348522782325745),\n",
       " ('touch_screen', 0.8328131437301636),\n",
       " ('phone', 0.8215099573135376),\n",
       " ('tablet', 0.808016836643219),\n",
       " ('touchscreen', 0.8049045205116272)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format('ycc_dataset_lg/w2vec_model/model.bin', binary=False)\n",
    "model.most_similar_cosmul('screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Happy New Year!\\n\\n⬇️Scotty’s Top DIY Tools:\\r...\n",
       "1    Hi Scotty I recently bought a 95 Camry automat...\n",
       "2    I really dislike those new grills as well.  Ha...\n",
       "3                             HAPPY NEW YEAR SCOTTY!!﻿\n",
       "4    Scotty Kilmer what do u think of a 2003 Chevy ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_label_corpusid(file_name):\n",
    "    \"\"\"Create row labels for each new data file\"\"\"\n",
    "    file_name = re.sub('downloads/', '', str(file_name))\n",
    "    file_name = re.sub('.json', '', file_name)\n",
    "    search_query, video_id = file_name.split('/')\n",
    "    return search_query, video_id\n",
    "\n",
    "corpus_large = []\n",
    "for file in glob('downloads/cartrends/*.json'):\n",
    "    corpus = pd.read_json(file, encoding='utf-8', lines=True)\n",
    "    search_query, video_id = df_label_corpusid(file)\n",
    "    corpus['search_query'] = search_query\n",
    "    corpus['video_id'] = video_id\n",
    "    corpus_large.append(corpus)\n",
    "\n",
    "corpus = pd.concat(corpus_large, ignore_index=True)\n",
    "corpus = pd.DataFrame(corpus)\n",
    "corpus.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import groupby\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def trim_whitespaces(text):\n",
    "    try:\n",
    "        text = \" \".join(text.split())\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_words(word):\n",
    "    # remove punctuation\n",
    "    word_map = word.maketrans(dict.fromkeys(string.punctuation))\n",
    "    word_clean = word.translate(word_map)\n",
    "    # put list back together into a sentence\n",
    "    return ' '.join([k for k, v in groupby(word_clean.split())])\n",
    "\n",
    "def reduce_words_with_repeated_chars(text):\n",
    "    findings = re.findall(r'(\\w)\\1{2,}', text)\n",
    "    for char in findings:\n",
    "        find = char + '{3,}'\n",
    "        #replace = char + '\\1' + '???'\n",
    "        replace = '???' + char + '???'\n",
    "        text = re.sub(find, repr(replace), text)\n",
    "\n",
    "    def remove_excessive_spaces(text):\n",
    "        # remove more than one space\n",
    "        text = re.sub(r'(\\s)\\1{1,}', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    # now we can remove the placeholders    \n",
    "    text = text.replace('\\'???','')\n",
    "    text = text.replace('???\\'','')\n",
    "    text = remove_excessive_spaces(text)\n",
    "    return text\n",
    "\n",
    "def standardize_text(df, header_name):\n",
    "    df[header_name] = df[header_name].str.replace(r\"http\\S+\", \"\")\n",
    "    df[header_name] = df[header_name].str.replace(r\"http\", \"\")\n",
    "    df[header_name] = df[header_name].str.replace(r\"@\\S+\", \"\")\n",
    "    df[header_name] = df[header_name].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[header_name] = df[header_name].str.replace(r\"@\", \"at\")\n",
    "    df[header_name] = df[header_name].str.lower()\n",
    "    return df\n",
    "\n",
    "def lemmatize_text(text_corpus, stopwords):\n",
    "    WordNet = WordNetLemmatizer()\n",
    "    if stopwords:\n",
    "        return ' '.join(\n",
    "            [WordNet.lemmatize(w) for w in text_corpus if w not in stopwords])\n",
    "    else:\n",
    "        return ' '.join([WordNet.lemmatize(w) for w in text_corpus])\n",
    "\n",
    "def normalize_text(df, header_name, stopwords=False):\n",
    "    if stopwords:\n",
    "        # adds 20 most uncommon|common words to stopword list\n",
    "        stop_words = build_stopwords(df, header_name)\n",
    "    else:\n",
    "        pass\n",
    "    df[header_name] = df[header_name].str.replace(r\"[^a-zA-Z]\", \" \")\n",
    "    df[header_name] = df[header_name].str.replace(r\"&lt;/?.*?&gt;\", \" &lt;&gt; \")\n",
    "    df[header_name] = df[header_name].str.replace(r\"(\\\\d|\\\\W)+\", \" \")\n",
    "    df[header_name] = df[header_name].str.split()\n",
    "    df[header_name] = df[header_name].apply(lambda x: lemmatize_text(x, stopwords=stop_words))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(df, header_name):\n",
    "    # first trim any whitespacing before applying contractions\n",
    "    df[header_name] = df[header_name].apply(lambda x: trim_whitespaces(x))\n",
    "    df[header_name] = df[header_name].apply(lambda x: contractions.fix(x))\n",
    "    return df\n",
    "# first step before removing contraction words\n",
    "corpus['text'] = corpus['text'].apply(lambda x: remove_whitespace(x))\n",
    "# removing contractions after striping extra white spaces works really well! TESTED!\n",
    "corpus['text'] = corpus['text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "\n",
    "# standardize text\n",
    "corpus = standardize_text(corpus, 'text')\n",
    "# normalize and lemmatize text\n",
    "corpus = normalize_text(corpus, 'text')\n",
    "# remove duplicates from corpus\n",
    "corpus['text'] = corpus['text'].apply(lambda x: remove_duplicate_words(x))\n",
    "# reduce repeated chars for text in strings\n",
    "corpus['text'] = corpus['text'].apply(lambda x: reduce_words_with_repeated_chars(x))\n",
    "# save test NOTE: TESTED!! IT WORKED AS PLANNED\n",
    "corpus.to_csv('final_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import groupby\n",
    "\n",
    "def remove_duplicate_words(word):\n",
    "    \"\"\"\n",
    "    This function removes punctuation! and then\n",
    "    Removes any repeted word in sequence.\n",
    "    Use this after preprocessing text\n",
    "    EXAMPLE:\n",
    "        >>> 0: 'hey! you are wrong very wrong! wrong!'\n",
    "        >>> df['text'].apply(lambda x: remove_duplicate_words(x))\n",
    "        >>> 0: 'hey you are wrong very wrong'\n",
    "    RETURNS:\n",
    "        Strings with no repeated words in sequence\n",
    "    \"\"\"\n",
    "    # remove punctuation\n",
    "    word_map = word.maketrans(dict.fromkeys(string.punctuation))\n",
    "    word_clean = word.translate(word_map)\n",
    "    # put list back together into a sentence\n",
    "    return ' '.join([k for k, v in groupby(word_clean.split())])\n",
    "\n",
    "# NOTE: This function does the job of removing duplicate words\n",
    "#corpus['no_duplicates'] = corpus['text'].apply(lambda x: remove_duplicate_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Before removing duplicate words `HEY! THA'S IT you are WRONG VERY WRONG! WRONG! ABOUT (BMW)`\n",
    "\n",
    ">- After removing duplicate words `HEY THAS IT you are WRONG VERY WRONG ABOUT BMW`\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "The words must be match exactly from one another `word_a == word_b`. It only removes words in sequence, I should use this function after preprocessing:\n",
    "    \n",
    "- removing contradictions\n",
    "    \n",
    "- punctuaction, lowercase, removing(tags, characters, and digits)\n",
    "    \n",
    "- lemmatization\n",
    "\n",
    "- remove duplicate words at this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_words_with_repeated_chars(text):\n",
    "    \"\"\"Original Code from Aurana\n",
    "    Reduces words with chars repeated more than 3 times to a single char. \n",
    "    Useful to replace words such as loooooooong by long. Be careful, \n",
    "    as it can change abreviations such as AAA to single A\n",
    "\n",
    "    USAGE\n",
    "    -----\n",
    "    df['text'] = df['text'].apply(\n",
    "            lambda x: reduce_words_with_repeated_chars(x))\n",
    "    \"\"\"\n",
    "    findings = re.findall(r'(\\w)\\1{2,}', text)\n",
    "    for char in findings:\n",
    "        find = char + '{3,}'\n",
    "        #replace = char + '\\1' + '???'\n",
    "        replace = '???' + char + '???'\n",
    "        text = re.sub(find, repr(replace), text)\n",
    "\n",
    "    def remove_excessive_spaces(text):\n",
    "        # remove more than one space\n",
    "        text = re.sub(r'(\\s)\\1{1,}', ' ', text)\n",
    "        # remove spaces in the beginning and in \n",
    "        # the end of the string\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    # Now we can remove the placeholders    \n",
    "    text = text.replace('\\'???','')\n",
    "    text = text.replace('???\\'','')\n",
    "    text = remove_excessive_spaces(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text_corpus):\n",
    "    WordNet = WordNetLemmatizer()\n",
    "    return ' '.join([WordNet.lemmatize(word) for word in text_corpus])\n",
    "\n",
    "def normalize_text(df, header_name):\n",
    "    \"\"\"Does the following in order\n",
    "    * removes punctuations\n",
    "    * converts to lowercase\n",
    "    * removes tags\n",
    "    * remove special characters & digits\n",
    "    * convert to list from string\n",
    "    * lemmatizes text\n",
    "    \"\"\"\n",
    "    df[header_name] = df[header_name].str.replace(r\"[^a-zA-Z]\", \" \")\n",
    "    df[header_name] = df[header_name].str.replace(r\"&lt;/?.*?&gt;\", \" &lt;&gt; \")\n",
    "    df[header_name] = df[header_name].str.replace(r\"(\\\\d|\\\\W)+\", \" \")\n",
    "    df[header_name] = df[header_name].str.split()\n",
    "    df[header_name] = df[header_name].apply(lambda x: lemmatize_text(x))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
