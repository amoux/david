{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Sequence\n",
    "\n",
    "from david.text import normalize_whitespace\n",
    "from david.text import get_sentiment_polarity\n",
    "from david.text import remove_punctuation\n",
    "from david.text import unicode_to_ascii\n",
    "from david.server import CommentsSql\n",
    "\n",
    "from wasabi import msg\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(batch, minlen=20):\n",
    "    batch_size = len(batch)\n",
    "    msg.warn(f\"* Preprocessing batch with {batch_size} samples...\")\n",
    "    comments = []\n",
    "    for sequence in batch:\n",
    "        text = normalize_whitespace(unicode_to_ascii(sequence))\n",
    "        if len(text) > minlen and text not in comments:\n",
    "            comments.append(text) \n",
    "    msg.good(f'* Removed {batch_size-len(comments)} comments'\n",
    "             f' from original batch size of {len(comments)}')\n",
    "    return comments\n",
    "\n",
    "def texts_to_sentences(texts, spacy_model='en_core_web_sm'):\n",
    "    msg.warn('* Transforming texts to sentences...')\n",
    "    sentences = []\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    for idx, doc in enumerate(nlp.pipe(texts)):\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text\n",
    "            polar = get_sentiment_polarity(remove_punctuation(text))\n",
    "            sentences.append((text, polar))\n",
    "    msg.good('* Done! here is some information about what happened.')\n",
    "    msg.info(f'* Before: {len(texts)} & After: {len(sentences)} ü§ñ')\n",
    "    return sentences\n",
    "\n",
    "def load_train_data(sentences) -> Tuple[List[str], List[str], List[Tuple[str, float]]]:\n",
    "    msg.warn('Converting sentences as training data...')\n",
    "    y_test = [] # comments with no sentiment score\n",
    "    x_train, x_labels = [], []\n",
    "    for sent, sentiment in sentences:\n",
    "        if sentiment == .0:\n",
    "            y_test.append((sent, sentiment))\n",
    "        else:\n",
    "            x_labels.append(1 if sentiment > 0 else 0)\n",
    "            x_train.append(sent)\n",
    "    return (x_train, x_labels, y_test)\n",
    "    msg.good(f'* Done! texts: {len(train_texts)}, labels: {len(train_labels)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m‚ö† * Preprocessing batch with 1483 samples...\u001b[0m\n",
      "\u001b[38;5;2m‚úî * Removed 46 comments from original batch size of 1437\u001b[0m\n",
      "\u001b[38;5;3m‚ö† * Transforming texts to sentences...\u001b[0m\n",
      "\u001b[38;5;2m‚úî * Done! here is some information about what happened.\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ * Before: 1437 & After: 3141 ü§ñ\u001b[0m\n",
      "\u001b[38;5;3m‚ö† Converting sentences as training data...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"%make a video%\"\n",
    "\n",
    "# fetch from a unbox-DB w/comments scrapped from all unbox-therapy's channel.\n",
    "db1 = CommentsSql('unbox')\n",
    "texts1 = [q.text for q in db1.fetch_comments(QUERY)]\n",
    "\n",
    "# fetch from v1-DB with random scrapped videos from various categories.\n",
    "db2 = CommentsSql('v1')\n",
    "texts2 = [q.text for q in db2.fetch_comments(QUERY)]\n",
    "\n",
    "# chaining the preprocessing pipeline on combined texts.\n",
    "train_data, train_labels, test_data = load_train_data(\n",
    "    texts_to_sentences(preprocess_texts(texts1+texts2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(vocab_size=2551)>\n"
     ]
    }
   ],
   "source": [
    "from david.tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer(document=train_data)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('video', 2), ('make', 3), ('the', 4), ('.', 5)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_vocab_to_frequency()\n",
    "tokenizer.bag_of_tokens(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 731), ('video', 538), ('make', 518), ('the', 490), ('.', 390)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [373, 7, 335, 23, 22, 1, 1015, 59, 1674, 451, 681]\n",
      "* ['hello', ',', 'world', '!', 'this', 'a', 'text', 'from', 'yt', 'comments', ':)']\n",
      "* hello, world! this a text from yt comments :)\n"
     ]
    }
   ],
   "source": [
    "string = \"hello, world! this a text from yt comments :)\"\n",
    "str2idx = tokenizer.convert_string_to_ids(string)\n",
    "idx2tok = tokenizer.convert_ids_to_tokens(str2idx)\n",
    "tok2str = tokenizer.convert_tokens_to_string(idx2tok)\n",
    "\n",
    "# you can convert from any input to another (all possible states available).\n",
    "for example in (str2idx, idx2tok, tok2str): print(f\"* {example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and loading your vocabulary.\n",
    "vectors_file = \"vecotors.pkl\"\n",
    "tokenizer.save_vectors(vectors_file)\n",
    "\n",
    "# Reload your vocab without having to pass the dataset again!\n",
    "# tokeniner = Tokenizer(vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from david.models import GloVe\n",
    "from david.text import largest_string_sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Loading vocab file from\n",
      "/home/ego/david_models/glove/glove.6B/glove.6B.100d.txt\u001b[0m\n",
      "\u001b[38;5;2m‚úî num-dim:(100), vocab-size: 2552\u001b[0m\n",
      "\u001b[38;5;2m‚úî *** embedding vocabulary ü§ó ***\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = GloVe.fit_embeddings(tokenizer.vocab_index, vocab_dim=\"100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 62, 100)           255200    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6201      \n",
      "=================================================================\n",
      "Total params: 261,401\n",
      "Trainable params: 6,201\n",
      "Non-trainable params: 255,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size, dimensions = glove_embeddings.shape\n",
    "seqmaxlen = largest_string_sequence(train_data, tokenizer.tokenize)\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, dimensions,\n",
    "                            weights=[glove_embeddings],\n",
    "                            input_length=seqmaxlen,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1132/1132 [==============================] - 1s 963us/step - loss: 0.6220 - acc: 0.6996\n",
      "Epoch 2/100\n",
      "1132/1132 [==============================] - 1s 645us/step - loss: 0.5339 - acc: 0.7456\n",
      "Epoch 3/100\n",
      "1132/1132 [==============================] - 1s 604us/step - loss: 0.4889 - acc: 0.7571\n",
      "Epoch 4/100\n",
      "1132/1132 [==============================] - 1s 706us/step - loss: 0.4540 - acc: 0.7880\n",
      "Epoch 5/100\n",
      "1132/1132 [==============================] - 1s 637us/step - loss: 0.4319 - acc: 0.8057\n",
      "Epoch 6/100\n",
      "1132/1132 [==============================] - 1s 615us/step - loss: 0.3993 - acc: 0.8242\n",
      "Epoch 7/100\n",
      "1132/1132 [==============================] - 1s 590us/step - loss: 0.3842 - acc: 0.8401\n",
      "Epoch 8/100\n",
      "1132/1132 [==============================] - 1s 480us/step - loss: 0.3644 - acc: 0.8534\n",
      "Epoch 9/100\n",
      "1132/1132 [==============================] - 1s 692us/step - loss: 0.3498 - acc: 0.8595\n",
      "Epoch 10/100\n",
      "1132/1132 [==============================] - 1s 828us/step - loss: 0.3335 - acc: 0.8657\n",
      "Epoch 11/100\n",
      "1132/1132 [==============================] - 1s 750us/step - loss: 0.3188 - acc: 0.8701\n",
      "Epoch 12/100\n",
      "1132/1132 [==============================] - 1s 575us/step - loss: 0.3096 - acc: 0.8905\n",
      "Epoch 13/100\n",
      "1132/1132 [==============================] - 1s 776us/step - loss: 0.2963 - acc: 0.8913\n",
      "Epoch 14/100\n",
      "1132/1132 [==============================] - 1s 703us/step - loss: 0.2866 - acc: 0.9011\n",
      "Epoch 15/100\n",
      "1132/1132 [==============================] - 1s 570us/step - loss: 0.2758 - acc: 0.9170\n",
      "Epoch 16/100\n",
      "1132/1132 [==============================] - 1s 665us/step - loss: 0.2669 - acc: 0.9134\n",
      "Epoch 17/100\n",
      "1132/1132 [==============================] - 1s 609us/step - loss: 0.2582 - acc: 0.9267\n",
      "Epoch 18/100\n",
      "1132/1132 [==============================] - 1s 866us/step - loss: 0.2493 - acc: 0.9329\n",
      "Epoch 19/100\n",
      "1132/1132 [==============================] - 1s 601us/step - loss: 0.2435 - acc: 0.9382\n",
      "Epoch 20/100\n",
      "1132/1132 [==============================] - 1s 722us/step - loss: 0.2348 - acc: 0.9373\n",
      "Epoch 21/100\n",
      "1132/1132 [==============================] - 1s 778us/step - loss: 0.2323 - acc: 0.9461\n",
      "Epoch 22/100\n",
      "1132/1132 [==============================] - 1s 620us/step - loss: 0.2226 - acc: 0.9443\n",
      "Epoch 23/100\n",
      "1132/1132 [==============================] - 1s 457us/step - loss: 0.2176 - acc: 0.9514\n",
      "Epoch 24/100\n",
      "1132/1132 [==============================] - 1s 583us/step - loss: 0.2117 - acc: 0.9496\n",
      "Epoch 25/100\n",
      "1132/1132 [==============================] - 1s 758us/step - loss: 0.2063 - acc: 0.9532\n",
      "Epoch 26/100\n",
      "1132/1132 [==============================] - 1s 584us/step - loss: 0.2002 - acc: 0.9558\n",
      "Epoch 27/100\n",
      "1132/1132 [==============================] - 1s 652us/step - loss: 0.1958 - acc: 0.9567\n",
      "Epoch 28/100\n",
      "1132/1132 [==============================] - 1s 598us/step - loss: 0.1908 - acc: 0.9594\n",
      "Epoch 29/100\n",
      "1132/1132 [==============================] - 1s 657us/step - loss: 0.1878 - acc: 0.9585\n",
      "Epoch 30/100\n",
      "1132/1132 [==============================] - 1s 570us/step - loss: 0.1823 - acc: 0.9602\n",
      "Epoch 31/100\n",
      "1132/1132 [==============================] - 1s 755us/step - loss: 0.1780 - acc: 0.9638\n",
      "Epoch 32/100\n",
      "1132/1132 [==============================] - 1s 819us/step - loss: 0.1752 - acc: 0.9629\n",
      "Epoch 33/100\n",
      "1132/1132 [==============================] - 1s 517us/step - loss: 0.1715 - acc: 0.9629\n",
      "Epoch 34/100\n",
      "1132/1132 [==============================] - 1s 634us/step - loss: 0.1667 - acc: 0.9673\n",
      "Epoch 35/100\n",
      "1132/1132 [==============================] - 1s 783us/step - loss: 0.1636 - acc: 0.9682\n",
      "Epoch 36/100\n",
      "1132/1132 [==============================] - 1s 674us/step - loss: 0.1612 - acc: 0.9700\n",
      "Epoch 37/100\n",
      "1132/1132 [==============================] - 1s 671us/step - loss: 0.1567 - acc: 0.9700\n",
      "Epoch 38/100\n",
      "1132/1132 [==============================] - 1s 700us/step - loss: 0.1537 - acc: 0.9717\n",
      "Epoch 39/100\n",
      "1132/1132 [==============================] - 1s 519us/step - loss: 0.1500 - acc: 0.9708\n",
      "Epoch 40/100\n",
      "1132/1132 [==============================] - 1s 797us/step - loss: 0.1476 - acc: 0.9700\n",
      "Epoch 41/100\n",
      "1132/1132 [==============================] - 1s 686us/step - loss: 0.1443 - acc: 0.9717\n",
      "Epoch 42/100\n",
      "1132/1132 [==============================] - 1s 737us/step - loss: 0.1418 - acc: 0.9735\n",
      "Epoch 43/100\n",
      "1132/1132 [==============================] - 1s 501us/step - loss: 0.1395 - acc: 0.9735\n",
      "Epoch 44/100\n",
      "1132/1132 [==============================] - 1s 743us/step - loss: 0.1366 - acc: 0.9753\n",
      "Epoch 45/100\n",
      "1132/1132 [==============================] - 1s 860us/step - loss: 0.1343 - acc: 0.9744\n",
      "Epoch 46/100\n",
      "1132/1132 [==============================] - 1s 577us/step - loss: 0.1319 - acc: 0.9726\n",
      "Epoch 47/100\n",
      "1132/1132 [==============================] - 1s 675us/step - loss: 0.1290 - acc: 0.9744\n",
      "Epoch 48/100\n",
      "1132/1132 [==============================] - 1s 698us/step - loss: 0.1274 - acc: 0.9744\n",
      "Epoch 49/100\n",
      "1132/1132 [==============================] - 1s 610us/step - loss: 0.1245 - acc: 0.9753\n",
      "Epoch 50/100\n",
      "1132/1132 [==============================] - 1s 736us/step - loss: 0.1237 - acc: 0.9779\n",
      "Epoch 51/100\n",
      "1132/1132 [==============================] - 1s 777us/step - loss: 0.1197 - acc: 0.9753\n",
      "Epoch 52/100\n",
      "1132/1132 [==============================] - 1s 835us/step - loss: 0.1185 - acc: 0.9779\n",
      "Epoch 53/100\n",
      "1132/1132 [==============================] - 1s 692us/step - loss: 0.1158 - acc: 0.9797\n",
      "Epoch 54/100\n",
      "1132/1132 [==============================] - 1s 565us/step - loss: 0.1144 - acc: 0.9814\n",
      "Epoch 55/100\n",
      "1132/1132 [==============================] - 1s 478us/step - loss: 0.1120 - acc: 0.9797\n",
      "Epoch 56/100\n",
      "1132/1132 [==============================] - 1s 724us/step - loss: 0.1104 - acc: 0.9814\n",
      "Epoch 57/100\n",
      "1132/1132 [==============================] - 1s 520us/step - loss: 0.1082 - acc: 0.9806\n",
      "Epoch 58/100\n",
      "1132/1132 [==============================] - 1s 601us/step - loss: 0.1074 - acc: 0.9806\n",
      "Epoch 59/100\n",
      "1132/1132 [==============================] - 1s 670us/step - loss: 0.1051 - acc: 0.9832\n",
      "Epoch 60/100\n",
      "1132/1132 [==============================] - 1s 706us/step - loss: 0.1047 - acc: 0.9832\n",
      "Epoch 61/100\n",
      "1132/1132 [==============================] - 1s 742us/step - loss: 0.1010 - acc: 0.9859\n",
      "Epoch 62/100\n",
      "1132/1132 [==============================] - 1s 889us/step - loss: 0.1006 - acc: 0.9832\n",
      "Epoch 63/100\n",
      "1132/1132 [==============================] - 1s 733us/step - loss: 0.0986 - acc: 0.9859\n",
      "Epoch 64/100\n",
      "1132/1132 [==============================] - 1s 679us/step - loss: 0.0967 - acc: 0.9859\n",
      "Epoch 65/100\n",
      "1132/1132 [==============================] - 1s 620us/step - loss: 0.0949 - acc: 0.9885\n",
      "Epoch 66/100\n",
      "1132/1132 [==============================] - 1s 657us/step - loss: 0.0941 - acc: 0.9867\n",
      "Epoch 67/100\n",
      "1132/1132 [==============================] - 1s 752us/step - loss: 0.0923 - acc: 0.9876\n",
      "Epoch 68/100\n",
      "1132/1132 [==============================] - 1s 691us/step - loss: 0.0905 - acc: 0.9885\n",
      "Epoch 69/100\n",
      "1132/1132 [==============================] - 1s 761us/step - loss: 0.0891 - acc: 0.9885\n",
      "Epoch 70/100\n",
      "1132/1132 [==============================] - 1s 716us/step - loss: 0.0886 - acc: 0.9894\n",
      "Epoch 71/100\n",
      "1132/1132 [==============================] - 1s 681us/step - loss: 0.0868 - acc: 0.9894\n",
      "Epoch 72/100\n",
      "1132/1132 [==============================] - 1s 637us/step - loss: 0.0852 - acc: 0.9894\n",
      "Epoch 73/100\n",
      "1132/1132 [==============================] - 1s 538us/step - loss: 0.0841 - acc: 0.9894\n",
      "Epoch 74/100\n",
      "1132/1132 [==============================] - 1s 592us/step - loss: 0.0825 - acc: 0.9903\n",
      "Epoch 75/100\n",
      "1132/1132 [==============================] - 1s 542us/step - loss: 0.0822 - acc: 0.9903\n",
      "Epoch 76/100\n",
      "1132/1132 [==============================] - 1s 453us/step - loss: 0.0801 - acc: 0.9912\n",
      "Epoch 77/100\n",
      "1132/1132 [==============================] - 1s 631us/step - loss: 0.0788 - acc: 0.9903\n",
      "Epoch 78/100\n",
      "1132/1132 [==============================] - 1s 718us/step - loss: 0.0776 - acc: 0.9903\n",
      "Epoch 79/100\n",
      "1132/1132 [==============================] - 1s 505us/step - loss: 0.0768 - acc: 0.9912\n",
      "Epoch 80/100\n",
      "1132/1132 [==============================] - 1s 654us/step - loss: 0.0754 - acc: 0.9920\n",
      "Epoch 81/100\n",
      "1132/1132 [==============================] - 1s 540us/step - loss: 0.0745 - acc: 0.9920\n",
      "Epoch 82/100\n",
      "1132/1132 [==============================] - 1s 608us/step - loss: 0.0737 - acc: 0.9912\n",
      "Epoch 83/100\n",
      "1132/1132 [==============================] - 1s 644us/step - loss: 0.0724 - acc: 0.9929\n",
      "Epoch 84/100\n",
      "1132/1132 [==============================] - 1s 667us/step - loss: 0.0710 - acc: 0.9938\n",
      "Epoch 85/100\n",
      "1132/1132 [==============================] - 1s 610us/step - loss: 0.0703 - acc: 0.9947\n",
      "Epoch 86/100\n",
      "1132/1132 [==============================] - 1s 851us/step - loss: 0.0690 - acc: 0.9956\n",
      "Epoch 87/100\n",
      "1132/1132 [==============================] - 1s 687us/step - loss: 0.0682 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "1132/1132 [==============================] - 1s 648us/step - loss: 0.0672 - acc: 0.9947\n",
      "Epoch 89/100\n",
      "1132/1132 [==============================] - 1s 731us/step - loss: 0.0664 - acc: 0.9973\n",
      "Epoch 90/100\n",
      "1132/1132 [==============================] - 1s 756us/step - loss: 0.0652 - acc: 0.9973\n",
      "Epoch 91/100\n",
      "1132/1132 [==============================] - 1s 649us/step - loss: 0.0645 - acc: 0.9956\n",
      "Epoch 92/100\n",
      "1132/1132 [==============================] - 1s 528us/step - loss: 0.0634 - acc: 0.9973\n",
      "Epoch 93/100\n",
      "1132/1132 [==============================] - 1s 677us/step - loss: 0.0626 - acc: 0.9956\n",
      "Epoch 94/100\n",
      "1132/1132 [==============================] - 1s 668us/step - loss: 0.0620 - acc: 0.9973\n",
      "Epoch 95/100\n",
      "1132/1132 [==============================] - 1s 691us/step - loss: 0.0608 - acc: 0.9965\n",
      "Epoch 96/100\n",
      "1132/1132 [==============================] - 1s 667us/step - loss: 0.0603 - acc: 0.9965\n",
      "Epoch 97/100\n",
      "1132/1132 [==============================] - 1s 729us/step - loss: 0.0596 - acc: 0.9973\n",
      "Epoch 98/100\n",
      "1132/1132 [==============================] - 1s 690us/step - loss: 0.0583 - acc: 0.9973\n",
      "Epoch 99/100\n",
      "1132/1132 [==============================] - 1s 598us/step - loss: 0.0577 - acc: 0.9965\n",
      "Epoch 100/100\n",
      "1132/1132 [==============================] - 1s 571us/step - loss: 0.0569 - acc: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f107b9f6590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Here we use the sequences from the tokenizer and we can now train our model\n",
    "sequences = tokenizer.document_to_sequences(train_data)\n",
    "padded_sequences = pad_sequences(list(sequences), padding=\"post\")\n",
    "model.fit(padded_sequences, train_labels, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nearest_emoji(score):\n",
    "    \"\"\"Find the nearest emoji matching a sentiment value.\"\"\"\n",
    "    EMOJI_EMOTIONS = {\n",
    "        99: 'üòç', 95: 'ü§ó', 90: 'üòÄ', 80: 'üòÅ', 70: 'üòä',\n",
    "        75: 'üòÖ', 55: 'üòë', 50: 'üò∂', 45: 'üòí', 35: 'üò¨',\n",
    "        30: 'üò≥', 25: 'üò§', 20: 'üò†', 10: 'üò°', 1: 'ü§¨',\n",
    "    }\n",
    "    array = np.asarray(list(EMOJI_EMOTIONS.keys()))\n",
    "    index = (np.abs(array - score)).argmin()\n",
    "    emoji_index = array[index]\n",
    "    if emoji_index:\n",
    "        return EMOJI_EMOTIONS[emoji_index]\n",
    "    return '‚ùì'\n",
    "\n",
    "def pad_input(string: str, maxlen: int) -> List[List[Sequence[int]]]:\n",
    "    \"\"\"New inputs need follow the same encoding steps as the dataset.\"\"\"\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    embedd = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return pad_sequences([embedd], maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "def predict(string: str, k=.5, model=model, maxlen=seqmaxlen) -> str:\n",
    "    \"\"\"Print the prediction for new inputs from the trained model.\"\"\"\n",
    "    embedd_input = pad_input(string, maxlen)\n",
    "    embedd_score = model.predict(embedd_input)[0]\n",
    "    if embedd_score[0] >= k: return (1, round(embedd_score[0]*100, 4))\n",
    "    else: return (0, round(embedd_score[0]*100, 4))\n",
    "\n",
    "def print_predict(string: str, k=.6):\n",
    "    label, score = predict(string, k=k)\n",
    "    emoji = nearest_emoji(score)\n",
    "    out = \"input: {} : {} ({})%\"\n",
    "    if label == 1:\n",
    "        out = out.format(string, f'<pos:({emoji})>', score)\n",
    "    else:\n",
    "        out = out.format(string, f'<neg:({emoji})>', score)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: hello there i am so glad this demo worked : <pos:(ü§ó)> (94.6934)%\n"
     ]
    }
   ],
   "source": [
    "print_predict(\"hello there i am so glad this demo worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: hello there! i am so glad this demo worked! : <pos:(ü§ó)> (95.1013)%\n"
     ]
    }
   ],
   "source": [
    "# the model is sensative to punctuation which makes sense (!!) displays exitment\n",
    "print_predict(\"hello there! i am so glad this demo worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we see that our model learned to detect - happy face `:)` and sad face `:(`\n",
    "\n",
    "- `hate` + `love` + `:)` => `(67.5018)%`\n",
    "\n",
    "- `love` + `hate` + `:)` => `(66.5365)%` \n",
    "\n",
    "- `hate` + `love` + `:(` => `(58.949)%`\n",
    "\n",
    "- `love` + `hate` + `:(` => `(57.8881)%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: I love this, but hate it :) : <pos:(üòä)> (66.5365)%\n",
      "input: I love this, but hate it :( : <neg:(üòë)> (57.8881)%\n",
      "\n",
      "input: I hate this, but love it :) : <pos:(üòä)> (67.5018)%\n",
      "input: I hate this, but love it :( : <neg:(üòë)> (58.949)%\n"
     ]
    }
   ],
   "source": [
    "emotion_face = {'pos': \":)\", 'neg': \":(\"}\n",
    "love_but_hate = \"I love this, but hate it {}\"\n",
    "hate_but_love = \"I hate this, but love it {}\"\n",
    "\n",
    "print_predict(love_but_hate.format(emotion_face[\"pos\"]))\n",
    "print_predict(love_but_hate.format(emotion_face[\"neg\"]))\n",
    "print()\n",
    "print_predict(hate_but_love.format(emotion_face[\"pos\"]))\n",
    "print_predict(hate_but_love.format(emotion_face[\"neg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# load the training data to test the model!\n",
    "y_data, _ = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ (old=0.0, new=79.2524)\n",
      " üòÅ - I want to buy the A50 it is a same look to A70 can u make a video on the A50\n",
      "\n",
      "üí¨ (old=0.0, new=91.3449)\n",
      " üòÄ - Can you please make a video on OLED BURNS?\n",
      "\n",
      "üí¨ (old=0.0, new=70.945)\n",
      " üòä - Make a video of snapdragon 655 processor\n",
      "\n",
      "üí¨ (old=0.0, new=90.9218)\n",
      " üòÄ - I've already had to replace 2 keys\n",
      "\n",
      "üí¨ (old=0.0, new=92.7388)\n",
      " ü§ó - Make a video about hp spectre folio\n",
      "\n",
      "üí¨ (old=0.0, new=82.5488)\n",
      " üòÅ - +??\n",
      "\n",
      "üí¨ (old=0.0, new=79.2417)\n",
      " üòÅ - And iPhone 11 is\n",
      "still not out for sale\n",
      "\n",
      "üí¨ (old=0.0, new=91.6293)\n",
      " üòÄ - you should make mare videos>>>>\n",
      "\n",
      "üí¨ (old=0.0, new=48.7595)\n",
      " üò∂ - Have you seen the 'Tecno phantom 9'\n",
      "\n",
      "üí¨ (old=0.0, new=82.5576)\n",
      " üòÅ - He‚Äôs only going back to Apple\n",
      "\n",
      "üí¨ (old=0.0, new=98.586)\n",
      " üòç - I like seeing you guys coming together to make a video.\n",
      "\n",
      "üí¨ (old=0.25, new=55.7028)\n",
      " üòë - ;)\n",
      "\n",
      "üí¨ (old=0.0, new=99.1161)\n",
      " üòç - Are you going to make a video about the 38\" LG Monitor?\n",
      "\n",
      "üí¨ (old=0.0, new=58.3881)\n",
      " üòë - please leave a comment here to Unbox Therapy to make a video on this issue.\n",
      "\n",
      "üí¨ (old=0.0, new=21.1462)\n",
      " üò† - You may search across the same topic on the internet and research over the issue.\n",
      "\n",
      "üí¨ (old=0.0, new=11.7892)\n",
      " üò° - Definitely nothing to celebrate or make a video about..\n",
      "\n",
      "üí¨ (old=0.0, new=99.9638)\n",
      " üòç - but you should try and collab with LTT or something and see if they can show you the linux gaming experience and make a video out of it.\n",
      "\n",
      "üí¨ (old=0.0, new=99.9494)\n",
      " üòç - @Lbc _ but a 4 inch display makes sense on a foldable phone that is .. why would you want anything bigger for simple things..\n",
      "\n",
      "üí¨ (old=0.0, new=34.3214)\n",
      " üò¨ - - Speakers:\n",
      "\n",
      "üí¨ (old=0.0, new=73.9812)\n",
      " üòÖ - Lew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for comment in random.sample(y_data, k=20):\n",
    "    old_score = get_sentiment_polarity(comment)\n",
    "    _, new_score = predict(comment)\n",
    "    label = nearest_emoji(new_score)\n",
    "    text = normalize_whitespace(comment)\n",
    "    print('üí¨ (old={}, new={})\\n {} - {}\\n'.format(\n",
    "        old_score, new_score, label, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End - Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save2path(dirname, filename):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    return os.path.join(dirname, filename)\n",
    "\n",
    "ROOT_DIR = 'ytc_sentiment'\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'model')\n",
    "VOCAB_DIR = os.path.join(ROOT_DIR, 'vocab')\n",
    "MODEL_FILE = save2path(MODEL_DIR, 'model.h5')\n",
    "VECTORS_FILE = save2path(VOCAB_DIR, 'vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model's and tokenizer's sources of information!\n",
    "model.save(MODEL_FILE) \n",
    "tokenizer.save_vectors(VECTORS_FILE)\n",
    "del_existing_model = False\n",
    "del_existing_tokenizer = False\n",
    "if del_existing_model:\n",
    "    del model\n",
    "if del_existing_tokenizer:\n",
    "    del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 62, 100)           255200    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6201      \n",
      "=================================================================\n",
      "Total params: 261,401\n",
      "Trainable params: 6,201\n",
      "Non-trainable params: 255,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# returns a compiled model identical to the previous one\n",
    "sentiment_model = load_model(MODEL_FILE)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(vocab_size=2551)>\n"
     ]
    }
   ],
   "source": [
    "# returns tokenizer identical to the previous one\n",
    "sentiment_tokenizer = Tokenizer(VECTORS_FILE)\n",
    "print(sentiment_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('video', 2), ('make', 3), ('the', 4), ('.', 5)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_tokenizer.bag_of_tokens(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ego/notebooks/random\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
