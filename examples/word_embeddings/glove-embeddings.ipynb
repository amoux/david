{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Model with GloVe Embeddings\n",
    "\n",
    "> In this notebook we will build a sentiment classifier model from comments (texts) scrapped from Youtube. And show how to use the `Tokenizer` for embedding a document and use it on `GloVe` pretrained weights.\n",
    "\n",
    "- Note: At the time this tokenizer does not fully integrate models like `Bert` and `Albert` as the `transformers` library does this! [huggingface-transformers github](https://github.com/huggingface/transformers)\n",
    "\n",
    "**Let's start by first loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment: This is very Good Way to Wake up myself from dreaming Fairy Life. Feeling Energetic Now.\n",
      "samples: 1600\n"
     ]
    }
   ],
   "source": [
    "from david.tokenizers import YTCommentsDataset, Tokenizer\n",
    "\n",
    "# For this demo, 1600 samples works but you can choose up to 6k samples.\n",
    "train_dataset, _ = YTCommentsDataset.split_train_test(2000, subset=0.8)\n",
    "print('comment:', train_dataset[0])\n",
    "print('samples:', len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(vocab_size=7828)>\n"
     ]
    }
   ],
   "source": [
    "# Contruct a Tokenizer object and pass a document to build the vocabulary.\n",
    "tokenizer = Tokenizer(document=train_dataset)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* tokens-to-index: [('this', 1), ('is', 2), ('very', 3), ('good', 4), ('way', 5)]\n",
      "* tokens-to-count: [('.', 2215), ('the', 2102), (',', 1613), ('i', 1297), ('to', 1286)]\n"
     ]
    }
   ],
   "source": [
    "def print_vocab(n=5):\n",
    "    # Lazy printing method to inspect the tokenizer's inner-workings.\n",
    "    print(\"* tokens-to-index: {}\\n* tokens-to-count: {}\".format(\n",
    "        tokenizer.bag_of_tokens(n), tokenizer.most_common(n)))\n",
    "\n",
    "print_vocab(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That was easy, the `Tokenizer` did all the hardwork for us! So is that it? Well, no. Lets see what else you can do with it, We did not import a `class` for one line of code!\n",
    "\n",
    "**- Next : ( `encoding / decoding` )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[519, 21, 567, 66, 1, 99, 216, 2726, 456, 61, 3378, 66, 847]\n"
     ]
    }
   ],
   "source": [
    "# Encoding strings to the vocabulary's index:\n",
    "text = \"hello, world! this text was embedded with youtube comments! 😁\"\n",
    "indexed_text = tokenizer.convert_string_to_ids(text)\n",
    "print(indexed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!', 'this', 'text', 'was', 'embedded', 'with', 'youtube', 'comments', '!', '😁']\n"
     ]
    }
   ],
   "source": [
    "# Decoding indexed sequences to tokens of string sequences:\n",
    "tokenized_index = tokenizer.convert_ids_to_tokens(indexed_text)\n",
    "print(tokenized_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world! this text was embedded with youtube comments! 😁\n"
     ]
    }
   ],
   "source": [
    "# Decode from index to string:\n",
    "print(tokenizer.convert_ids_to_string(indexed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember, you can **`convert`** from any input state to whatever `x` state you want with one call. The `Tokenizer` has you covered!\n",
    "\n",
    "```bash\n",
    "convert_ids_to_string, convert_ids_to_tokens\n",
    "convert_string_to_ids, convert_string_to_tokens\n",
    "convert_tokens_to_ids, convert_tokens_to_string\n",
    "```\n",
    "\n",
    "**- Next : `encoding the dataset`**\n",
    "\n",
    "> In order to do any cool *'Machine Learning'* our dataset needs to meet the following steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠  Warning: Vocabulary's index has not been transformed to frequency.\n",
      "Applying the needed requirements for fitting the documents. Calling\n",
      "`self.vocab_index_to_frequency` for you...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Which the tokenizer will do for us :) transform the dataset to embeddings\n",
    "sequences = tokenizer.document_to_sequences(document=train_dataset)\n",
    "\n",
    "# The method yields each item but our dataset is small and memory wont do us harm!\n",
    "sequences = list(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is that? We forgot to explicitly call the method to.. let me just show you!**\n",
    "\n",
    "- before calling: `tokenizer.document_to_sequences`.\n",
    "\n",
    "```python\n",
    "* tokens-to-index: [('this', 1), ('is', 2), ('very', 3), ('good', 4), ('way', 5)]\n",
    "* tokens-to-count: [('.', 2215), ('the', 2102), (',', 1613), ('i', 1297), ('to', 1286)]\n",
    "```\n",
    "\n",
    "- after calling: `tokenizer.document_to_sequences`.\n",
    "\n",
    "```python\n",
    "* tokens-to-index: [('.', 1), ('the', 2), (',', 3), ('i', 4), ('to', 5)]\n",
    "* tokens-to-count: [('.', 2215), ('the', 2102), (',', 1613), ('i', 1297), ('to', 1286)]\n",
    "```\n",
    "\n",
    "> Long story short, the `Tokenizer` did not do any stupid magic behind our backs. The message is there to let us know - For next-time if we need to do `x` before this. \n",
    "\n",
    "Furthermore, If you look at the order on present **`tokens-to-index`** - you will notice that tokens `['.', 'the', ',', 'i']` are aligned in relation to the order frequency - which are the tokens present in  **`tokens-to-count`**. We can say the vocabulary is now aliged to its `term` frequecy. Lastly, It is not a hard problem, as it is simply re-indexing the tokens to the term frequency found in the dataset. This `indexing` thing is your friend and you need to know when it happens!\n",
    "\n",
    "**Next : `( loading / saving )`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* tokens-to-index: [('.', 1), ('the', 2), (',', 3), ('i', 4), ('to', 5)]\n",
      "* tokens-to-count: [('.', 2215), ('the', 2102), (',', 1613), ('i', 1297), ('to', 1286)]\n"
     ]
    }
   ],
   "source": [
    "print_vocab(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ INFO: Using `self.save_vectors` and `self.load_vectors` is\n",
      "recommended over simply saving the vocabulary as it saves both states from the\n",
      "vocab_index and vocab_count dict objects Both which improve the tokenizer's\n",
      "features.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# How to save the vocabulary from the tokenizer.\n",
    " # You need handle your own path to files and directory.\n",
    "VOCAB_BASE = \"vocab\"\n",
    "if not os.path.exists(VOCAB_BASE):\n",
    "    os.makedirs(VOCAB_BASE, exist_ok=True)\n",
    "VOCAB_FILE = os.path.join(VOCAB_BASE, \"vocab.pkl\")\n",
    "VECTORS_FILE = os.path.join(VOCAB_BASE, \"vectors.pkl\")\n",
    "\n",
    "# one way to save the vocab if you really just want the vocabulary.\n",
    "tokenizer.save_vocabulary(VOCAB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7e91fc4b0caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# This way we can 'restore' our instance without having to load the dataset again!\n",
    "tokenizer.save_vectors(VECTORS_FILE)\n",
    "del tokenizer # just to prove my point\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tokenizer(vocab_size=7828)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(VECTORS_FILE)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!', 'this', 'text', 'was', 'embedded', 'with', 'youtube', 'comments', '!', '😁']\n"
     ]
    }
   ],
   "source": [
    "# Everything is loaded like nothing happended including the vocab as frequency of terms\n",
    "print(tokenizer.convert_string_to_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* tokens-to-index: [('.', 1), ('the', 2), (',', 3), ('i', 4), ('to', 5)]\n",
      "* tokens-to-count: [('.', 2215), ('the', 2102), (',', 1613), ('i', 1297), ('to', 1286)]\n"
     ]
    }
   ],
   "source": [
    "print_vocab(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2215, 1), ('the', 2102, 2), (',', 1613, 3), ('i', 1297, 4), ('to', 1286, 5)]\n"
     ]
    }
   ],
   "source": [
    "# last trick, you can also get the vocab as [(token, freq, index)]\n",
    "vocab_vectors = tokenizer.vocab_to_vectors()[:5]\n",
    "print(vocab_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding sequences with GloVe's pretrained weights\n",
    "\n",
    "> The `david.tokenizer.Tokenizer` class made the preprocessing and encoding a lot easier, but it gets easier to use the `GloVe` embeddings with one line of code! We can simply pass the indexed vocabulary and choose the vocab dimension we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Loading vocab file from\n",
      "/home/ego/david_models/glove/glove.6B/glove.6B.100d.txt\u001b[0m\n",
      "\u001b[38;5;2m✔ num-dim:(100), vocab-size: 7829\u001b[0m\n",
      "\u001b[38;5;2m✔ *** embedding vocabulary 🤗 ***\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from david.models import GloVe\n",
    "glove_embeddings = GloVe.fit_embeddings(tokenizer.vocab_index, vocab_dim=\"100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7829, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thats it! we now have embedded our sequences with glove's vocab weights.\n",
    "glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment labels: [1, 1, 0, 0, 0]\n",
      "dataset / labels: (1600, 1600)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Sequence\n",
    "from david.text import get_sentiment_polarity\n",
    "\n",
    "def get_sentiment_labels(sequences: List[Sequence[int]]) -> List[int]:\n",
    "    '''Overkill for obtaining sentiment scores. But its an easy\n",
    "    way to show how we can use the tokenizer to decode the embedded\n",
    "    sequences back to strings.'''\n",
    "    labels = []\n",
    "    for sequence in sequences:\n",
    "        string = tokenizer.convert_ids_to_string(sequence)\n",
    "        polarity = get_sentiment_polarity(string)\n",
    "        labels.append(1 if polarity > 0 else 0)\n",
    "    return labels\n",
    "\n",
    "sentiment_labels = get_sentiment_labels(sequences=sequences)\n",
    "print('sentiment labels:', sentiment_labels[:5])\n",
    "print('dataset / labels:', (len(sentiment_labels), len(sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the sentiment model (neural-network) with Keras\n",
    "\n",
    "> After preprocessing and encoding the dataset from youtube comments - we can begin with creating a Sequential model from the `glove embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from david.text import largest_string_sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 798, 100)          782900    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 79800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 79801     \n",
      "=================================================================\n",
      "Total params: 862,701\n",
      "Trainable params: 79,801\n",
      "Non-trainable params: 782,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size, dimensions = glove_embeddings.shape\n",
    "largest_input_length = largest_string_sequence(document=train_dataset,\n",
    "                                               tokenizer=tokenizer.tokenize) \n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, dimensions,\n",
    "                            weights=[glove_embeddings],\n",
    "                            input_length=largest_input_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6894 - acc: 0.5825\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 1s 882us/step - loss: 0.5284 - acc: 0.7594\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4575 - acc: 0.8238\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4136 - acc: 0.8481\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3753 - acc: 0.8863\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3463 - acc: 0.9044\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 2s 951us/step - loss: 0.3226 - acc: 0.9075\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 1s 886us/step - loss: 0.3007 - acc: 0.9212\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2820 - acc: 0.9375\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2641 - acc: 0.9438\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2495 - acc: 0.9538\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 1s 882us/step - loss: 0.2377 - acc: 0.9569\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2246 - acc: 0.9644\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2144 - acc: 0.9681\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2032 - acc: 0.9725\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 2s 959us/step - loss: 0.1934 - acc: 0.9731\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1852 - acc: 0.9769\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1779 - acc: 0.9819\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1707 - acc: 0.9819\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1628 - acc: 0.9825\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1564 - acc: 0.9856\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1507 - acc: 0.9856\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1442 - acc: 0.9875\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1387 - acc: 0.9875\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1333 - acc: 0.9887\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1300 - acc: 0.9862\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 2s 960us/step - loss: 0.1249 - acc: 0.9919\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1198 - acc: 0.9919\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 2s 959us/step - loss: 0.1150 - acc: 0.9931\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1117 - acc: 0.9944\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1074 - acc: 0.9931\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.1040 - acc: 0.9944\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 2s 945us/step - loss: 0.1011 - acc: 0.9956\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0969 - acc: 0.9944\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 2s 994us/step - loss: 0.0950 - acc: 0.9937\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 1s 937us/step - loss: 0.0905 - acc: 0.9956\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 2s 980us/step - loss: 0.0887 - acc: 0.9969\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0851 - acc: 0.9962\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0831 - acc: 0.9969\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 2s 943us/step - loss: 0.0803 - acc: 0.9969\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 1s 935us/step - loss: 0.0776 - acc: 0.9969\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0759 - acc: 0.9962\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 2s 953us/step - loss: 0.0734 - acc: 0.9975\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0706 - acc: 0.9975\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0682 - acc: 0.9981\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 2s 985us/step - loss: 0.0663 - acc: 0.9987\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0642 - acc: 0.9987\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0625 - acc: 0.9987\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 2s 995us/step - loss: 0.0608 - acc: 0.9987\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0590 - acc: 0.9994\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0577 - acc: 0.9994\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0559 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0543 - acc: 0.9994\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0529 - acc: 0.9994\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0511 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 2s 967us/step - loss: 0.0500 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 2s 942us/step - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0470 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 1s 833us/step - loss: 0.0461 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0436 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0421 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0390 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0363 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 2s 957us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 2s 951us/step - loss: 0.0336 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 2s 995us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0288 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0280 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0274 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 1s 917us/step - loss: 0.0269 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0249 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0225 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 2s 988us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 2s 983us/step - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 2s 981us/step - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 1s 887us/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.0164 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f61f4105190>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Here we use the sequences from the tokenizer and we can now train our model\n",
    "padded_sequences = pad_sequences(sequences, largest_input_length, padding=\"post\")\n",
    "model.fit(padded_sequences, sentiment_labels, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment on new inputs\n",
    "\n",
    "> Below I created some helper methods for predicting sentiment from new inputs (texts not in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input(string: str) -> List[List[Sequence[int]]]:\n",
    "    \"\"\"New inputs need follow the same encoding steps as the dataset.\"\"\"\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    embedd = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    maxlen = largest_input_length  # including the largest input value.\n",
    "    return pad_sequences([embedd], maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "\n",
    "def predict(string: str, k=.6, model=model) -> str:\n",
    "    \"\"\"Print the prediction for new inputs from the trained model.\"\"\"\n",
    "    embedd_input = pad_input(string)\n",
    "    embedd_score = model.predict(embedd_input)[0]\n",
    "    out_template = \"input: {} : {} -> ({})%\"\n",
    "\n",
    "    if embedd_score[0] >= k:\n",
    "        out_template = out_template.format(\n",
    "            string, \"<POSITIVE>\", round(embedd_score[0]*100, 4))\n",
    "    else:\n",
    "        out_template = out_template.format(\n",
    "            string, \"<NEGATIVE>\", round(embedd_score[0]*100, 4))\n",
    "\n",
    "    return out_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input: hello there i am so glad this demo worked : <POSITIVE> -> (66.8464)%'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"hello there i am so glad this demo worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input: hello there! i am so glad this demo worked! : <POSITIVE> -> (74.7259)%'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can see the difference of training the model\n",
    "# with/without punctuation. The Tokenizer helped the model\n",
    "# with detecting punctuation and emoji's as part of semantical context.\n",
    "\n",
    "predict(\"hello there! i am so glad this demo worked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: input: I love this, but hate it 😁! : <POSITIVE> -> (88.8346)%\n",
      "2: input: I hate this, but love it 😡! : <POSITIVE> -> (85.3823)%\n"
     ]
    }
   ],
   "source": [
    "# in this demo we dont handle emoji's in this demo, but it is part of the models context.\n",
    "# There is multiple ways (rule-based or model-based) to include emoji's as part of sentiment scores\n",
    "\n",
    "emoji = {'pos': '😁!', 'neg': '😡!'}\n",
    "t1 = predict(\"I love this, but hate it {}\".format(emoji[\"pos\"]))\n",
    "t2 = predict(\"I hate this, but love it {}\".format(emoji[\"neg\"]))\n",
    "print('1:', t1)\n",
    "print('2:', t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: America is a falling empire, these moves are the only things they have left to rely on. It's just a matter of time before Samsung come to their senses and join forces with Huawei and create their own OS and ecosystem. who is to say that the American government won't come for Samsung if they get to the position Huawei is in. Huawei is it for the long game. Already they have been laying the groundwork all over Africa and Asia and eastern Europe. Google and all western brands are fucked in the long run. This is the future while the west is on a population decline. Africa 1,679 Billion 2030 – Africa 4.3 Billion 2100 India 1,527 Billion 2030 – India 1.6 Billion 2100 China 1,415 Billion 2030 – China 1 Billion 2100 All of Asia combine 4,922 Billion 2030 - Asia 4,888 Billion 2100 https://www.populationpyramid.net/africa/2100/ : <NEGATIVE> -> (0.012)%\n",
      "\n",
      "input: Thank you very much for this. This is really great :) : <POSITIVE> -> (99.9406)%\n",
      "\n",
      "input: just went through a bad breakup , lost my closest friends after that. It was difficult until i found your video to find myself again. thank you so much : <NEGATIVE> -> (1.2294)%\n",
      "\n",
      "input: Real World to Hollywood ain't no such thing as the perfect life or perfect person : <POSITIVE> -> (99.6482)%\n",
      "\n",
      "input: Trump will retaliate with an army of mexicans and black americans. Woot! : <NEGATIVE> -> (2.1209)%\n",
      "\n",
      "input: this shit has been haunting me for so long now, i think they are bery close to brainwash me. : <NEGATIVE> -> (9.7896)%\n",
      "\n",
      "input: i really really wanna chase mi dream and work for it bit somethings holding me back i have no idea what thai bullshit is :( : <NEGATIVE> -> (1.3395)%\n",
      "\n",
      "input: @IamHKer I've been working in IT for 20 years, how about you? : <NEGATIVE> -> (5.1389)%\n",
      "\n",
      "input: MADERCHOD NE BEWKOOF BANA DIYA BHEN KE LAWDE NE ISKI MA KI CHUT MAIN SAMSUNG IPHONE WAALO KA LAWDA MADERCHOD THUMBNAILS FAKE HAI, VIDEO FAKE HAI, TERI MA KI CHUT BHI FAKE HAI : <NEGATIVE> -> (0.6116)%\n",
      "\n",
      "input: after some testign for myself i discovered it. and made the function a lot smaller and a lot more easy: _________________________________ from Question import Question questions = [ Question(\"what color are apples? \\n(a) red/green\\n(b) Purple\\n(c) Orange\\n\\n\", \"a\" ), Question(\"what color are Bananas? \\n(a) Teal\\n(b) Magenta\\n(c) Yellow\\n\\n\", \"c\" ), Question(\"what color are strawberries? \\n(a) Yellow\\n(b) red\\n(c) Blue\\n\\n\", \"b\" ), ] for question in questions: answer = input(question.prompt) if answer == question.answer: score = score + 1 print(\"You got \" + str(score) + \"/\" + str(len(questions)) + \" Correct\" ) ____________________________ am i missing something by deleting some code? : <POSITIVE> -> (100.0)%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_dataset = random.sample(train_dataset, k=10)\n",
    "for comment in test_dataset:\n",
    "    pred = predict(comment)\n",
    "    print(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model\n",
    "\n",
    "> Steps of how easy it is to save and load a trained model with `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 798, 100)          782900    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 79800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 79801     \n",
      "=================================================================\n",
      "Total params: 862,701\n",
      "Trainable params: 79,801\n",
      "Non-trainable params: 782,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "MODEL_DIR = 'model'\n",
    "MODEL_FILE = 'sentiment.h5'\n",
    "# deletes the existing model\n",
    "delete_existing_model = False\n",
    "\n",
    "if not os.path.exists(MODEL_DIR): os.mkdir(MODEL_DIR)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "    \n",
    "# creates a HDF5 file\n",
    "model.save(MODEL_PATH)\n",
    "if delete_existing_model:\n",
    "    del model\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "sentiment_model = load_model(MODEL_PATH)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input: hello, world! this a text from the loaded model! : <POSITIVE> -> (95.4739)%'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_sentiment(string: str, k=0.60):\n",
    "    return predict(string, k=k, model=sentiment_model)\n",
    "\n",
    "predict_sentiment(\"hello, world! this a text from the loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
